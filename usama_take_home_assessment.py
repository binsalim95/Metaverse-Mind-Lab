# -*- coding: utf-8 -*-
"""Usama - Take Home Assessment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IBywRWvMrIF8aNpCwTLUmJMGjerJxczu

# Take Home Assessment - Fake News Bot

For this project I will clean the data, use a TF-idf vectorizer on text data with English Stop words and use an XG Boosted Random Forest model to classify the text data. I'm going to use a tree based model as opposed to a neural net since literature supports that Tree based outperform neural nets on tabular data.
"""

import pandas as pd

df1 = pd.read_csv('/content/Data/labels.csv')
df1.head()

df2 = pd.read_csv('/content/Data/test.csv')
df2.head()

df3 = pd.read_csv('/content/Data/train.csv')
df3.head()

"""# Cleaning Pipeline"""

# Cleaning Pipeline
dfn = df3.convert_dtypes()
dfn = dfn.dropna()
dfn['text'] = dfn['text'].str.replace('\n', '')

import re

#Text Processing Functions - Getting rid of Contractions, stop words, and tokenizing sentences

def decontracted(phrase):
    # specific
    phrase = re.sub(r"won\’t", "will not", phrase)
    phrase = re.sub(r"can\’t", "can not", phrase)
    phrase = re.sub(r"good morning", "good_morning", phrase)
    phrase = re.sub(r"thank you", "thank_you", phrase)
    phrase = re.sub(r"didn’t", "did not", phrase)

    # general
    phrase = re.sub(r"n\’t", " not", phrase)
    phrase = re.sub(r"\’re", " are", phrase)
    phrase = re.sub(r"\’s", " is", phrase)
    phrase = re.sub(r"\’d", " would", phrase)
    phrase = re.sub(r"\’ll", " will", phrase)
    phrase = re.sub(r"\’t", " not", phrase)
    phrase = re.sub(r"\’ve", " have", phrase)
    phrase = re.sub(r"\’m", " am", phrase)
    return phrase

#Stop word removal and further processing
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
stop_words = set(stopwords.words('english'))
stop_words.update({'thank_you', 'good_morning', 'morning', 'hi', 'good', 'could', 'thank', 'hello', 'mute', 'button', 'go', "look" , "need" , "think", "really", "make", "know", "get"})

def remove_stopwords(Words):
  x = Words.split()
  y = [word for word in x if word not in stop_words]
  return (' '.join(y))

import string
def remove_punctuations(text):
    for punctuation in string.punctuation:
        text = text.replace(punctuation, '')
    return text

dfn['text_clean'] = dfn['text'].apply(lambda x: decontracted(str(x)))
dfn['text_clean'] = dfn['text_clean'].astype(str).str.replace('–', '')
dfn['text_clean'] = dfn['text_clean'].astype(str).str.replace('U.S.', 'United States')
dfn['text_clean'] = dfn['text_clean'].str.strip()
dfn["text_clean"] = dfn['text_clean'].apply(lambda x: remove_punctuations(x))
dfn["text_clean"] = dfn["text_clean"].apply(lambda x: x.lower())
dfn["text_clean"] = dfn["text_clean"].apply(lambda x: remove_stopwords(x))





"""# TF - IDF Vectorizer"""

from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer

dataset = dfn["text_clean"]
tfIdfVectorizer=TfidfVectorizer(lowercase=True, max_features=90, max_df=0.7, min_df=1, stop_words="english", use_idf=True)
tfIdf = tfIdfVectorizer.fit_transform(dataset)

'''
df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=["TF-IDF"])
df = df.sort_values('TF-IDF', ascending=False)
print (df.head(50))
'''

"""# XG Boost"""

import xgboost as xgb

from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error
from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split

y = dfn['label']

xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
xgb_model.fit(tfIdf, y)

y_pred = xgb_model.predict(tfIdf)

import numpy as np
pred= np.reshape(y_pred, (y_pred.shape[0],  1))

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score

accuracy = accuracy_score(y.astype(int), y_pred.astype(int))
print("Accuracy: %.2f%%" % (accuracy * 100.0))

f1 = f1_score(y.astype(int), y_pred.astype(int), average='weighted')
print("f1: %.2f%%" % (f1 * 100.0))

"""# Testing Data"""

#y_pred = xgb_model.predict(tfIdf)

df4 = pd.merge(df1, df2, on="id")

df4.head()

dfn2 = df4.convert_dtypes()
dfn2 = dfn2.dropna()
dfn2['text'] = dfn2['text'].str.replace('\n', '')

dfn2['text_clean'] = dfn2['text'].apply(lambda x: decontracted(str(x)))
dfn2['text_clean'] = dfn2['text_clean'].astype(str).str.replace('–', '')
dfn2['text_clean'] = dfn2['text_clean'].astype(str).str.replace('U.S.', 'United States')
dfn2['text_clean'] = dfn2['text_clean'].str.strip()
dfn2["text_clean"] = dfn2['text_clean'].apply(lambda x: remove_punctuations(x))
dfn2["text_clean"] = dfn2["text_clean"].apply(lambda x: x.lower())
dfn2["text_clean"] = dfn2["text_clean"].apply(lambda x: remove_stopwords(x))

dataset = dfn2["text_clean"]
tfIdfVectorizer=TfidfVectorizer(lowercase=True, max_features=90, max_df=0.7, min_df=1, stop_words="english", use_idf=True)
tfIdf2 = tfIdfVectorizer.fit_transform(dataset)

"""# XG Boost"""

y_pred = xgb_model.predict(tfIdf2)
Y = dfn2['label']

accuracy = accuracy_score(Y.astype(int), y_pred.astype(int))
print("Accuracy: %.2f%%" % (accuracy * 100.0))

'''
63.85%
61.95%


Accuracy: 89.43%
Accuracy: 83.73%
'''